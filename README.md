# ğŸ§  OmniBrain MCP

**Give your AI a brain that never forgets.**

A persistent, cloud-synced memory server for AI assistants using the **Model Context Protocol (MCP)**. Works across Claude, Cursor, Windsurf, VS Code, Gemini CLI â€” all sharing the same memory in real-time.

> **Use case:** You're building a project in Cursor. You switch to Claude Code. It already knows what Cursor did â€” your architecture decisions, file changes, preferences â€” everything. Zero context repetition.

---

## ğŸš€ One-Command Setup

```bash
git clone https://github.com/saibighnesh/omnibrain-mcp.git
cd omnibrain-mcp
npm install
npm run setup
```

The interactive setup wizard will guide you through:
- ğŸ”‘ Validating your `serviceAccountKey.json`
- ğŸ‘¤ Setting up your unique User ID
- ğŸ—ï¸ Building the project
- ğŸ“‹ Generating **ready-to-paste configs** for your preferred MCP client

## ğŸ’» Supported Clients
The setup wizard automatically generates the exact configuration you need for:
- Antigravity (Google / Gemini)
- Claude Desktop
- Cursor IDE
- Windsurf / Codeium
- VS Code (via the Roo Code / Cline extension)
- Claude Code CLI

Just follow the prompt at the end of `npm run setup`!

---

### Optional: Semantic Search
Add **any one** API key to enable AI-powered similarity search:

```env
# Pick ONE â€” whichever you already have:
GEMINI_API_KEY=your_key_here          # Free (recommended)
OPENAI_API_KEY=your_key_here          # text-embedding-3-small
COHERE_API_KEY=your_key_here          # embed-english-v3.0

# Optional overrides:
EMBEDDING_PROVIDER=openai             # Force a specific provider
OPENAI_BASE_URL=https://your-api.com  # For Azure, Ollama, LM Studio, etc.
OPENAI_MODEL=text-embedding-3-large   # Custom model name
```

> **Auto-detection:** If no `EMBEDDING_PROVIDER` is set, the server picks the first key it finds (Gemini â†’ OpenAI â†’ Cohere). No key = smart fuzzy search (still works great).

---

## ğŸ§° All 15 Tools

| Tool | What It Does |
|---|---|
| `add_memory` | Save a new memory with optional tags |
| `get_memory` | Fetch a single memory by ID |
| `get_all_memories` | List memories (paginated, pinned first) |
| `search_memories` | Smart fuzzy search by text and/or tags |
| `semantic_search` | AI-powered similarity search (Gemini embeddings) |
| `update_memory` | Edit an existing memory |
| `delete_memory` | Delete a memory |
| `pin_memory` | Pin/unpin important memories |
| `add_memories` | Bulk add multiple memories at once |
| `delete_memories` | Bulk delete by IDs |
| `link_memories` | Create relationships between memories |
| `unlink_memories` | Remove relationships |
| `export_memories` | Export all memories as JSON |
| `import_memories` | Import memories (merge or replace) |
| `cleanup_expired` | Remove memories past their TTL |

### Example Prompts
```
"Remember that this project uses Next.js 15 with App Router"
â†’ add_memory(fact: "Project uses Next.js 15 with App Router", tags: ["tech-stack"])

"What do you know about my project setup?"
â†’ search_memories(query: "project setup")

"Link the auth memory to the API memory"
â†’ link_memories(sourceId: "abc", targetId: "xyz")
```

---



## âœ¨ Key Features

| Feature | Description |
|---|---|
| ğŸ”„ **Multi-IDE Sync** | Same memory across Claude, Cursor, Windsurf, VS Code, Gemini CLI |
| ğŸ§  **Semantic Search** | Find memories by meaning, not just keywords (Gemini AI) |
| ğŸ“Œ **Pinned Memories** | Prioritize important facts |
| ğŸ”— **Memory Links** | Connect related memories into a knowledge graph |
| â° **Auto-Expiry (TTL)** | Memories can auto-delete after a set time |
| ğŸ“¦ **Bulk Operations** | Add/delete many memories at once |
| ğŸ“¤ **Import/Export** | Full backup and restore |
| ğŸ” **User Isolation** | Each `--user-id` gets its own private namespace |
| â˜ï¸ **Cloud Sync** | Real-time sync across all devices |
| ğŸ”„ **Auto-Retry** | Exponential backoff on transient errors |

---

## ğŸ“ Project Structure

```
omnibrain-mcp/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts        # Entry point
â”‚   â”œâ”€â”€ config.ts       # CLI args + env vars
â”‚   â”œâ”€â”€ server.ts       # MCP tool registration (15 tools)
â”‚   â”œâ”€â”€ store.ts        # FirestoreMemoryStore (CRUD + search)
â”‚   â”œâ”€â”€ embeddings.ts   # Gemini AI embeddings
â”‚   â”œâ”€â”€ logger.ts       # Structured logger
â”‚   â””â”€â”€ types.ts        # TypeScript interfaces
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ store.test.ts   # 19 unit tests
â”‚   â””â”€â”€ smoke.mjs       # Live Firestore smoke test
â”œâ”€â”€ setup.mjs           # One-command setup script
â”œâ”€â”€ serviceAccountKey.json  # (gitignored) Firebase credentials
â””â”€â”€ package.json
```

---

## ğŸ§ª Testing

```bash
npm test              # Unit tests (19 tests, all mocked)
node tests/smoke.mjs  # Smoke test against live Firestore
```

---

## âš™ï¸ Environment Variables

| Variable | Default | Description |
|---|---|---|
| `USER_ID` | â€” | Alternative to `--user-id` CLI flag |
| `GEMINI_API_KEY` | â€” | Gemini embeddings (free, default) |
| `OPENAI_API_KEY` | â€” | OpenAI embeddings |
| `OPENAI_BASE_URL` | `api.openai.com` | Custom endpoint (Azure, Ollama, LM Studio) |
| `OPENAI_MODEL` | `text-embedding-3-small` | Custom embedding model |
| `COHERE_API_KEY` | â€” | Cohere embeddings |
| `EMBEDDING_PROVIDER` | auto-detect | Force: `gemini`, `openai`, `cohere` |
| `LOG_LEVEL` | `info` | `debug`, `info`, `warn`, or `error` |

---

## ğŸ› ï¸ Troubleshooting

| Problem | Solution |
|---|---|
| `Missing serviceAccountKey.json` | Download from Firebase Console â†’ Project Settings â†’ Service Accounts |
| `Missing User ID` | Add `--user-id=your-name` to the command |
| `PERMISSION_DENIED` | Enable Cloud Firestore API in Google Cloud Console |
| Server disconnected in IDE | Run `npm run build`, restart your IDE |
| Memories not syncing | Ensure all clients use the **same** `--user-id` |

## ğŸ” Competitors & Alternatives (Why OmniBrain?)

The ecosystem for MCP memory servers is growing. **OmniBrain** differentiates itself by focusing on **cross-IDE cloud synchronization** using Firebase Firestore, allowing you to switch between Claude Desktop, Cursor, and CLI without losing context.

If you are looking for other specific approaches to AI memory management, consider these excellent alternatives:
*   **Official MCP Reference Memory Server:** A standard reference implementation focused on knowledge graph-based persistent memory.
*   **Redis Agent Memory Server:** A dual-memory system providing working and long-term memory utilizing Redis for semantic search and fast local access.
*   **meMCP:** Uses TF-IDF-based semantic indexing for fact retrieval and a continuous learning quality scoring system.
*   **Memory Bank MCP (Cline/Roo Code):** Inspired by Cline, this focuses on localized project workspace remote memory bank management isolated per project.
*   **doobidoo/mcp-memory-service:** A REST API-focused service optimized for AI agent pipelines like LangGraph, CrewAI, and AutoGen using local embeddings.

---

## ğŸ“„ License

MIT
